{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "x-bfht2Sp04O"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cv2\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DsoZD004OfRW"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Feature Extraction (Deep Features)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# Feature Extraction (Deep Features)\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from skimage import feature\n",
    "from skimage.feature import hog\n",
    "from imutils import paths\n",
    "from google.colab.patches import cv2_imshow\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "# Load Inception_v3 pretrained on ImageNet dataset\n",
    "W=224\n",
    "H=224\n",
    "#model = InceptionV3(include_top=False, weights='imagenet', pooling='avg', input_tensor=Input(shape=(W,H,3)))\n",
    "#model = ResNet50(include_top=False, weights='imagenet', pooling='avg', input_tensor=Input(shape=(W,H,3)))\n",
    "model = Xception(include_top=False, weights='imagenet', pooling='avg')\n",
    "\n",
    "# List of paths \n",
    "import os\n",
    "file_list=[]\n",
    "file_list.append(os.listdir(r\"/home/jorge/OneDrive/GitHub/University_Projects/Machine Learning/Class11/fake\"))\n",
    "file_list.append(os.listdir(r\"/home/jorge/OneDrive/GitHub/University_Projects/Machine Learning/Class11/real\"))\n",
    "\n",
    "# general path\n",
    "path='/home/jorge/OneDrive/GitHub/University_Projects/Machine Learning/Class11/'\n",
    "\n",
    "# list of classes\n",
    "class_names=['fake', 'real'] \n",
    "\n",
    "X_deep = []\n",
    "y = []\n",
    "\n",
    "# Gera lista com as imagens\n",
    "for classes_files, classe in zip (file_list, range(2)):\n",
    "    for i in range(len(classes_files)):\n",
    "      name= str(path) + str(class_names[classe]) + str('/') + str(classes_files[i]) \n",
    "#      print(name)\n",
    "      y.append(classe)\n",
    "      \n",
    "# Carrega a imagem, pré-processa e inclui em uma lista \n",
    "      imagem = cv2.imread(name)\n",
    "      img = cv2.resize(imagem,(W,H))\n",
    "      xd = image.img_to_array(img)\n",
    "      xd = np.expand_dims(xd, axis=0)\n",
    "      xd = preprocess_input(xd)\n",
    "      X_deep.append(xd)\n",
    "\n",
    "# Extrai características usando o modelo profundo pré-treinado (deep learning)\n",
    "X_deep = np.asarray(X_deep)\n",
    "X_deep = X_deep.reshape(X_deep.shape[0], W, H, 3)\n",
    "X =  model.predict(X_deep)      \n",
    "\n",
    "# Salva as características extraídas em um csv (um vetor de valores para cada imagem)\n",
    "df = pd.DataFrame(X)\n",
    "df.to_csv('X_im.csv', header=False, index=False)\n",
    "\n",
    "# Salva y que contém a classe de cada imagem\n",
    "df_class = pd.DataFrame(y)\n",
    "df_class.to_csv('y_im.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5rl3X6iTAb0"
   },
   "outputs": [],
   "source": [
    "# Exemplo de como carregar os arquivos gerados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Labels\n",
    "y = pd.read_csv('y_im.csv', header=None)\n",
    "y=y.to_numpy()\n",
    "y=np.ravel(y)\n",
    "print(y.shape)\n",
    "\n",
    "# deep features\n",
    "X = pd.read_csv('X_im.csv', header=None)\n",
    "X=X.to_numpy()\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoYOqTQqPiH9"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Definição dos parâmetros a serem avaliados\n",
    "parameters=[{'hidden_layer_sizes':[1000, (1000,500)], \n",
    "             'activation':['relu', 'logistic'],}]\n",
    "\n",
    "# Separar uma parte dos dados para validação\n",
    "#X, X_val, y, y_val=train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "\n",
    "# definição da técnica a ser utilizada\n",
    "mlp=MLPClassifier(random_state=46, max_iter=500, hidden_layer_sizes=(1000, 500))\n",
    "\n",
    "#gs=GridSearchCV(mlp, parameters, cv=5)\n",
    "#gs.fit(X_val, y_val)\n",
    "#mlp=gs.best_estimator_\n",
    "#print(\"Melhores parâmetros:\", gs.best_params_)\n",
    "\n",
    "#from tabulate import tabulate\n",
    "#df=gs.cv_results_\n",
    "#print(tabulate(df, headers='keys', tablefmt='psql'))\n",
    "\n",
    "result=model_selection.cross_val_score(mlp, X, y, cv=5)\n",
    "print(\"Accuracy: %.5f\" % result.mean())\n",
    "print(\"std: %.5f\" % result.std())\n",
    "y_pred=model_selection.cross_val_predict(mlp, X, y, cv=5)\n",
    "ma=confusion_matrix(y, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(ma)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
